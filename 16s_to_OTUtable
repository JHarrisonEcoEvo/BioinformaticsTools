#!/bin/bash
#To cite anything here, see the USEARCH webpage and cite appropriately.
#I copied the pipeline from the USEARCH documentation and heavily edited
#J. Harrison Jan 16, 2018
#
# 
# #IMPORTANT PLEASE READ!!!
# #This script will output a summary txt called Processing_Summary in a dir called out/
# #within the working dir. This has important QC info so check it out.
# #This script assumes one starts in the dir that has all unpacked fastqs to process
# #The script requires a dir to be present called db/ that has a comparison database of your chosing
# #You can change the path to this database in Step 3
# 
# #You will want to make sure the primer stripping step (step 4) has the correct number of 
# #bases specified to be removed
# #this will change depending on what primers you are using
# 
# #This script assumes demultiplexed reads
# 
# #YOU WILL NEED a few extra scripts to be in the working dir. These scripts should be 
# #passed along with this script to any users

# #rmv_fungal_matches.pl

# #usage bash fastq_to_OTU.sh 

# #You will want to generate taxonomic hypotheses for the OTUs/ZOTUs generated here
# #Then you will want to remove unwanted taxa from the OTU table (e.g. host reads)

#troubleshooting. 
#if you did rename the usearch binary you will get errors
#this is because the computer is looking for "usearch8.1.1861_i86linux32" but the binary is actually called
#usearch
##just rename the binary or the calls to it in this script

#needs
#need to think about sequences that differ by just an indel
#think about crosstalk

##############################
#What this script does
#1. Check and see if we have any samples with a lot of errors (this is set at 2 for now)
#2. merges your paired end reads
#3. checks that reads are oriented in the same way
#4. removes bp where the primer binds, as these are prone to errors in base calling
#5. Sequence filtering
#6. Find unique read sequences and abundances
#7. Make OTUs and ZOTUS
#8. Match original reads to OTUs
#9. Make an OTU table
#10. Calculate summary statistics and add those to summary file
#11. Clean up working dir
#############################


#here the script removes any directories called "out" in the working directory
#and makes a new directory called out
rm -rf out
mkdir out
touch out/Processing_Summary.txt

gunzip *fastq.gz 

# #######################################
# ####Step 1. Examine quality of reads
# #######################################

mkdir fastq_info

for fq in *fastq
do
  usearch -fastx_info $fq -output fastq_info/${fq}info
done

#make a summary txt file that has number of expected errors per sample
grep "^EE" fastq_info/* > fastq_info/expected_error_summary.txt

#cut out just the floating point estimates of expected errors
grep -o "EE mean [0-9].[0-9]" fastq_info/expected_error_summary.txt | cut -d " " -f3 > EE.txt

counter=0
#initialize array of bad samples
badsamples[0]=""
while read p; do
	counter=$((counter+1))
	if (( $(echo "$p > 2" | bc -l) )); then
	 	toadd=`echo $counter`
	 	 badsamples+=(`echo $counter`)
	 	 echo $toadd
	 fi
done < EE.txt #note the strange way to pass an in file here

rm EE.txt

#get the length of the array. if more than one then we have a problem sample
numbad=`echo ${#distro[@]}`

#if problem samples are present, then tell us which those are
#if [ "$numbad > 1" ]; then
if (( $(echo "$numbad > 1" | bc -l) )); then
	echo "The following samples have more than two expected errors, perhaps keep an eye on them:" >> out/Processing_Summary.txt

	#for each element in this array do...
	for i in `echo "${badsamples[@]}"`
	do
		sed -n ${i}p fastq_info/expected_error_summary.txt >> out/Processing_Summary.txt
	done
else
	echo "No samples had more than 2 expected errors. yay!" >> out/Processing_Summary.txt
fi

# #######################################
# ####Step 2. Merge reads
# #######################################

# Merge paired reads
# Add sample name to read label (using the -relabel option). 
#Sample name is just the file name
#this defaults to 10 threads, or number of cores, whichever is less
#I doubt you will need to run more cores here very often as this is not slow

for f in *R1*
do
	fname=$(basename $f)
	fname2=${fname/R1/R2}
	
	usearch -fastq_mergepairs ${fname} -reverse ${fname2} -fastqout ${fname}.merged.fq -relabel $fname
	  #note that we skip the step below because in the 32 bit version we cannot
	  #do other processing steps on such a big file. 
	  #instead we have to do most things separately for each file
	  #cat $Sample.merged.fq >> all.merged.fq
done

# #######################################
# ####Step 3. Check reads are oriented the same way
# #######################################

#note they should be from MiSeq, but worth doublechecking. This may need to be reconsidered
#if we try other sequencing techniques for which I am unfamiliar

#Note that I am just checking for a few samples, as all samples should be the same
#note the database for comparison may need a new path, or be changed if you have a different db

cat `ls *merged* | head -n 5` > testFiles.fq

usearch -orient testFiles -db db/rdp_16s_v16_sp.fa -tabbedout orient_out.txt

#this should show most reads are + or ? (the latter being ones that didn't match anything in the db)

echo "Sequence orientation for the first 5 samples." >> out/Processing_Summary.txt 

echo `cut -f2 orient_out.txt | sort | uniq -c`  >> out/Processing_Summary.txt

rm -rf orient_out.txt
rm -rf testFiles.fq

# #######################################
# ####Step 4. Remove primer binding region
# #######################################

# # Strip primers (V4F is 18, V4R is 20) (ITS1F is 22, ITS2R is 20)
# # this will be important to doublecheck, as this can vary by primer pair
 
for f in *merged.fq
do
	usearch -fastx_truncate $f -stripleft 18 -stripright 20 -fastqout ${f}stripped.fq
done

#Note we don't do any more length trimming because we are using merged reads

#######################################
####Step 5. Sequence filtering
#######################################

#Note that we do this after merging so we can leverage info from both reads when calling bases
#If you are using a sequence technology prone to errors, then best to rethink this step
#or at least look into the output more carefully, 
#see https://www.drive5.com/usearch/manual/pipe_readprep_filter.html

for f in *stripped.fq
do
	usearch -fastq_filter $f -fastq_maxee 1.0 -fastaout ${f}.filtered.fa
done

#######################################
####Step 6. Find unique read sequences and abundances
#######################################
# 
# EDIT: :::: this has to be a two step process because of limitations of 32 bit version
# NOTE THIS messes up the zotu calling bc that function uses sizeout as a way to figure out errors
# 
# 
# for f in *filtered.fa
# do
# 	usearch -fastx_uniques $f -sizeout -fastaout ${f}_uniques.fa
# done
# 
# echo "about to cat may take some time"
# 
# cat *_uniques.fa > combined_uniques.fa
# 
# usearch -fastx_uniques combined_uniques.fa -sizeout -fastaout uniqueSequences.fa

cat *filtered.fa> combined_filtered.fa

usearch -fastx_uniques combined_filtered.fa -sizeout -fastaout uniqueSequences.fa

# #######################################
# ####Step 7. make OTUs and ZOTUs
# #######################################


## # Make 97% OTUs and filter chimeras
#note we remove singletons here

usearch -cluster_otus uniqueSequences.fa -otus otus97.fa -relabel Otu -minsize 2

# # Denoise: predict biological sequences and filter chimeras
#also called ZOTUs, or ESVs..exact sequence variants
usearch -unoise3 uniqueSequences.fa -zotus zotus.fa
 
##################################################
# Step 8. Match original reads to OTUs
##################################################

#dont use filtered reads bc we can recover some info from them when using 97% match

for f in *merged.fq
do
	usearch -usearch_global $f -db otus97.fa -id 0.97 -threads 32 -strand plus -uc ${f}_97.uc
done

cat *97.uc > all_ucs_97.uc

#This removes sequences that didn't match anything
perl rmv_matches.pl all_ucs_97.uc

mv all_matches.uc all_matches97.uc

#Do same thing for zotus
#note search exact doesn't work for some reason...Not sure why

for f in *merged.fq
do
	usearch -usearch_global $f -db zotus.fa -id 1 -threads 32 -strand plus -uc ${f}_zotus.uc
done

cat *zotus.uc > all_ucs_zotus.uc

#This removes sequences that didn't match anything
perl rmv_matches.pl all_ucs_zotus.uc

mv all_matches.uc all_matchesZOTUs.uc

# ##################################################
# # Step 9. make OTU table
# ##################################################

#this writes the OTU tables to the out/ dir

Rscript uc_to_OTUtable.R all_matchesZOTUs.uc all_matches97.uc

 ##################################################
 # Step 10. Summary info calculation
 ##################################################

#Count number of original reads
echo "Original number of forward raw reads (unmerged, unfiltered)" >> out/Processing_Summary.txt
echo `grep "@M0" *R1*.fastq | wc -l` >> out/Processing_Summary.txt

echo "Original number of reverse raw reads (unmerged, unfiltered)" >> out/Processing_Summary.txt
echo `grep "@M0" *R2*.fastq | wc -l` >> out/Processing_Summary.txt

#number of reads that merged successfully
echo "Number of reads that merged successfully" >> out/Processing_Summary.txt
echo `grep "@*fastq" *merged.fq | wc -l` >> out/Processing_Summary.txt

#number of reads that passed filtering
echo "Number of merged reads that passed filtering" >> out/Processing_Summary.txt
echo `grep ">" *filtered.fa | wc -l` >> out/Processing_Summary.txt

#number of unique sequences
echo "Number of unique sequences" >> out/Processing_Summary.txt
echo `grep ">" uniqueSequences.fa | wc -l` >> out/Processing_Summary.txt

#number of Otus
echo "Number of OTUS (97% similarity threshold)" >> out/Processing_Summary.txt
echo "Many of these will be non-target organism (i.e. host)" >> out/Processing_Summary.txt
echo `grep ">" otus97.fa | wc -l` >> out/Processing_Summary.txt

echo "Number of ZOTUS" >> out/Processing_Summary.txt
echo "Many of these will be non-target organism (i.e. host)" >> out/Processing_Summary.txt
echo `grep ">" zotus.fa | wc -l` >> out/Processing_Summary.txt

#number of reads that matched an OTU
echo "Number of sequences that matched an 97% OTU" >> out/Processing_Summary.txt
echo "Many of these will be non-target" >> out/Processing_Summary.txt
echo `wc -l all_matches97.uc` >> out/Processing_Summary.txt

echo "Number of sequences that matched an ZOTU" >> out/Processing_Summary.txt
echo "Many of these will be non-target" >> out/Processing_Summary.txt
echo `wc -l all_matchesZOTUs.uc` >> out/Processing_Summary.txt

# ##################################################
# # Step 11. Clean up files
# ##################################################

rm -rf *merged*

gzip *fastq
mkdir rawreads

mv *fastq.gz rawreads
mv all_matches* out/
mv otus97.fa out/
mv zotus.fa out/

rm -rf all_ucs*uc
rm -rf uniqueSequences.fa
rm -rf combined_filtered.fa

tar -czf fastq_info.tar.gz  fastq_info/
rm -rf fastq_info/


